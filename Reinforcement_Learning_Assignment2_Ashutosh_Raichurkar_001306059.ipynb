{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2                                       \n",
    "## (Ashutosh Raichurkar 001306059)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "![](projectw2.gif)\n",
    "## Abstract\n",
    "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment)\n",
    "\n",
    "## Actions\n",
    " |Num||Action|\n",
    " |---||:----:|\n",
    " |0||Move Left|\n",
    " |1||Move Down|\n",
    " |2||Move Right|\n",
    " |3||Move Up|\n",
    " \n",
    " ## Reward\n",
    " Reward is 0 for every step taken, 0 for falling in the hole, 1 for reaching the final goal\n",
    " \n",
    " \n",
    " ## Grid\n",
    " \n",
    " The letters in the Grid have the following significance\n",
    "- S: initial state\n",
    "- F: frozen lake\n",
    "- H: hole\n",
    "- G: the goal\n",
    "- Square: indicates the current position of the player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcemnt Learning\n",
    "## Model\n",
    "![](Reinforcement_L.png)\n",
    "\n",
    "## \n",
    "![](RL.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an environment for the game\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the states, the actions and the size of the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Action: 4\n",
      "Number of States: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Action:\",action_space_size)\n",
    "print(\"Number of States:\",state_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: The size of the Q-Table is equal is 16 x 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets undersand the Environment by playing the game manually and calculating the reward for each action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "action = 1\n",
    "new_state, reward, done, info = env.step(action)#execute the action\n",
    "print(\"Reward:\",reward)\n",
    "env.render()\n",
    "action = 2\n",
    "new_state, reward, done, info = env.step(action)#execute the action\n",
    "print(\"Reward:\",reward)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline performance. How well did your RL Q-Learning do on your problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a baseline model to play the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "max_steps_per_episode = 99 #max number of steps that the agent is allowed to take while playing the game\n",
    "\n",
    "learning_rate = 0.7    # The value of alpha = learning rate in the bellman's equation \n",
    "discount_rate = 0.8     # Dicount rate is the penalty after the moving from one state to the other, in order to complete the game in less number of steps\n",
    "\n",
    "exploration_rate = 1          #Exploration rate\n",
    "max_exploration_rate = 1      #max Exploration Rate\n",
    "min_exploration_rate = 0.01   #Min Exploration Rate\n",
    "exploration_decay_rate = 0.01 #Decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes\n",
      "\n",
      "1000 :  0.21500000000000016\n",
      "2000 :  0.34900000000000025\n",
      "3000 :  0.35000000000000026\n",
      "4000 :  0.34200000000000025\n",
      "5000 :  0.3710000000000003\n",
      "Total Score 0.32540000000000024\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []  #hold all the rewards from each episode\n",
    "#q learning algo\n",
    "for episode in range(num_episodes): #every thing that happens in a episode\n",
    "    state = env.reset() #reset the state of envt to start\n",
    "    done = False #track if the episode is finished\n",
    "    rewards_current_episode = 0 \n",
    "    \n",
    "    for step in range(max_steps_per_episode): #every thing that happens in a single time step in a episode\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) #whether the agent eill explore \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) # exploit envt\n",
    "        else:\n",
    "            action = env.action_space.sample() #explore\n",
    "        \n",
    "        # action is choosen\n",
    "        new_state, reward, done, info = env.step(action)#execute the action\n",
    "        \n",
    "        #update q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        state = new_state #set current state to new\n",
    "        rewards_current_episode += reward #update the reward\n",
    "        if done == True: #check if last action ended the episode\n",
    "            break\n",
    "            \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode) #decay exploration rate\n",
    "    rewards_all_episodes.append(rewards_current_episode) #append the episode reward\n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "clt=0\n",
    "\n",
    "print(\"Average reward per thousand episodes\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    clt=clt+sum(r/1000)\n",
    "    count += 1000\n",
    "print(\"Total Score\",clt/(num_episodes/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ans: By implementing the baseline model we get reward for 5000 episodes is 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By changing the hyper-parameters like Learning Rate, Decay Rate and Hyper Parameter to improve the reward return by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.03\n",
    "discount_rate = 0.91\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes\n",
      "\n",
      "1000 :  0.05000000000000004\n",
      "2000 :  0.1560000000000001\n",
      "3000 :  0.36600000000000027\n",
      "4000 :  0.4930000000000004\n",
      "5000 :  0.5600000000000004\n",
      "6000 :  0.5520000000000004\n",
      "7000 :  0.6590000000000005\n",
      "8000 :  0.6470000000000005\n",
      "9000 :  0.6970000000000005\n",
      "10000 :  0.6370000000000005\n",
      "Total Score 0.48170000000000035\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []  #hold all the rewards from each episode\n",
    "#q learning algo\n",
    "for episode in range(num_episodes): #every thing that happens in a episode\n",
    "    state = env.reset() #reset the state of envt to start\n",
    "    done = False #track if the episode is finished\n",
    "    rewards_current_episode = 0 \n",
    "    \n",
    "    for step in range(max_steps_per_episode): #every thing that happens in a single time step in a episode\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) #whether the agent eill explore \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) # exploit envt\n",
    "        else:\n",
    "            action = env.action_space.sample() #explore\n",
    "        \n",
    "        # action is choosen\n",
    "        new_state, reward, done, info = env.step(action)#execute the action\n",
    "    \n",
    "        #update q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        #print(n,q_table[state, action],state,action)\n",
    "        state = new_state #set current state to new\n",
    "        rewards_current_episode += reward #update the reward\n",
    "        if done == True: #check if last action ended the episode\n",
    "            break\n",
    "            \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode) #decay exploration rate\n",
    "    rewards_all_episodes.append(rewards_current_episode) #append the episode reward\n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "clt=0\n",
    "\n",
    "print(\"Average reward per thousand episodes\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    clt=clt+sum(r/1000)\n",
    "print(\"Total Score\",clt/(num_episodes/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting a nested For-Loop in order to find the optimal value of learning Rate, Discount Rate and Decay Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_all_episodes = []\n",
    "ctr=0\n",
    "alpha = [0.05,0.07,0.09]\n",
    "dk = [0.7,0.8,0.99]\n",
    "pk = [0.001,0.002,0.003]\n",
    "for x in alpha:\n",
    "    for y in dk:\n",
    "        for z in pk:\n",
    "            learning_rate = x\n",
    "            discount_rate = y\n",
    "            exploration_decay_rate = z\n",
    "            for episode in range(num_episodes): #every thing that happens in a episode\n",
    "                state = env.reset() #reset the state of envt to start\n",
    "                done = False #track if the episode is finished\n",
    "                rewards_current_episode = 0 \n",
    "                for step in range(max_steps_per_episode): #every thing that happens in a single time step in a episode\n",
    "                    exploration_rate_threshold = random.uniform(0, 1) #whether the agent eill explore \n",
    "                    if exploration_rate_threshold > exploration_rate:\n",
    "                        action = np.argmax(q_table[state,:]) # exploit envt\n",
    "                    else:\n",
    "                        action = env.action_space.sample() #explore\n",
    "                    new_state, reward, done, info = env.step(action)#execute the action\n",
    "                    \n",
    "                    q_table[state, action] =  q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "                    \n",
    "                    state = new_state #set current state to new\n",
    "                    rewards_current_episode += reward\n",
    "                    if done == True:\n",
    "                        break\n",
    "                exploration_rate = min_exploration_rate + \\\n",
    "                (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "                \n",
    "                rewards_all_episodes.append(rewards_current_episode)\n",
    "                ctr = ctr+1\n",
    "            \n",
    "            rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "            count = 1000\n",
    "            alg = np.sum(rewards_per_thosand_episodes)\n",
    "            print(\"--------------------------\")\n",
    "            print(alg/ctr)\n",
    "            print(\"Learning Rate\",x)\n",
    "            print(\"Discount Rate\",y)\n",
    "            print(\"Decay Rate\",z)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent with the optimal values for Learning Rate, Decay Rate and Disocunt rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 99 #max number of steps that the agent is allowed to take while playing the game\n",
    "\n",
    "learning_rate = 0.07   # The value of alpha = learning rate in the bellman's equation \n",
    "discount_rate = 0.99     # Dicount rate is the penalty after the moving from one state to the other, in order to complete the game in less number of steps\n",
    "\n",
    "exploration_rate = 1          #Exploration rate\n",
    "max_exploration_rate = 1      #max Exploration Rate\n",
    "min_exploration_rate = 0.01   #Min Exploration Rate\n",
    "exploration_decay_rate = 0.001 #Decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes\n",
      "\n",
      "1000 :  0.06200000000000005\n",
      "2000 :  0.22600000000000017\n",
      "3000 :  0.3980000000000003\n",
      "4000 :  0.5700000000000004\n",
      "5000 :  0.6490000000000005\n",
      "6000 :  0.6830000000000005\n",
      "7000 :  0.6730000000000005\n",
      "8000 :  0.6700000000000005\n",
      "9000 :  0.7040000000000005\n",
      "10000 :  0.6860000000000005\n",
      "Total Score 0.5321000000000005\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []  #hold all the rewards from each episode\n",
    "#q learning algo\n",
    "for episode in range(num_episodes): #every thing that happens in a episode\n",
    "    state = env.reset() #reset the state of envt to start\n",
    "    done = False #track if the episode is finished\n",
    "    rewards_current_episode = 0 \n",
    "    \n",
    "    for step in range(max_steps_per_episode): #every thing that happens in a single time step in a episode\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) #whether the agent eill explore \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) # exploit envt\n",
    "        else:\n",
    "            action = env.action_space.sample() #explore\n",
    "        \n",
    "        # action is choosen\n",
    "        new_state, reward, done, info = env.step(action)#execute the action\n",
    "        #n=n+1\n",
    "        #update q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        #print(n,q_table[state, action],state,action)\n",
    "        state = new_state #set current state to new\n",
    "        rewards_current_episode += reward #update the reward\n",
    "        if done == True: #check if last action ended the episode\n",
    "            break\n",
    "            \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode) #decay exploration rate\n",
    "    rewards_all_episodes.append(rewards_current_episode) #append the episode reward\n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "clt=0\n",
    "\n",
    "print(\"Average reward per thousand episodes\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    clt=clt+sum(r/1000)\n",
    "print(\"Total Score\",clt/(num_episodes/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letting the Agent play with Q-values from the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "You got ine the hole\n",
      "Number of steps 11\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "You have Reached the Goal\n",
      "Number of steps 16\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "You have Reached the Goal\n",
      "Number of steps 22\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "You have Reached the Goal\n",
      "Number of steps 69\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "You have Reached the Goal\n",
      "Number of steps 17\n",
      "Score 4.0\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "episode_steps = []\n",
    "rewards_current_episode = 0\n",
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    print(\"****************************************************\")\n",
    "    print(\"EPISODE \", episode)\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(q_table[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        rewards_current_episode += reward #update the reward\n",
    "        \n",
    "        if done:\n",
    "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "        \n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"You have Reached the Goal\")\n",
    "            else: \n",
    "                print(\"You got ine the hole\")\n",
    "            # We print the number of step it took.\n",
    "            print(\"Number of steps\", step)\n",
    "            break\n",
    "        state = new_state\n",
    "    episode_steps.append(step)\n",
    "env.close()\n",
    "print(\"Score\",rewards_current_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "***you fell in the hole***\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):\n",
    "    state = env.reset() #Reseting the environment\n",
    "    done = False\n",
    "    print(\"*****Episode****\", episode+1,\"******\\n\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait = True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state, reward , done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait = True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"***You reached the goal\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"***you fell in the hole***\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait = True)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Average number of steps taken per episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of steps per episode 27.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Number of steps per episode\",np.mean(episode_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try a policy other than maxQ(s',a'). How did it change the baseline performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "max_steps_per_episode = 99 #max number of steps that the agent is allowed to take while playing the game\n",
    "\n",
    "learning_rate = 0.7    # The value of alpha = learning rate in the bellman's equation \n",
    "discount_rate = 0.8     # Dicount rate is the penalty after the moving from one state to the other, in order to complete the game in less number of steps\n",
    "\n",
    "exploration_rate = 1          #Exploration rate\n",
    "max_exploration_rate = 1      #max Exploration Rate\n",
    "min_exploration_rate = 0.01   #Min Exploration Rate\n",
    "exploration_decay_rate = 0.01 #Decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes\n",
      "\n",
      "1000 :  0.0\n",
      "2000 :  0.0\n",
      "3000 :  0.0\n",
      "4000 :  0.0\n",
      "5000 :  0.0\n",
      "Total Score 0.0\n",
      "Average Number of steps per episode 34.0204\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []  #hold all the rewards from each episode\n",
    "#q learning algo\n",
    "for episode in range(num_episodes): #every thing that happens in a episode\n",
    "    state = env.reset() #reset the state of envt to start\n",
    "    done = False #track if the episode is finished\n",
    "    rewards_current_episode = 0 \n",
    "    \n",
    "    for step in range(max_steps_per_episode): #every thing that happens in a single time step in a episode\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) #whether the agent eill explore \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmin(q_table[state,:]) # exploit envt\n",
    "        else:\n",
    "            action = env.action_space.sample() #explore\n",
    "        \n",
    "        # action is choosen\n",
    "        new_state, reward, done, info = env.step(action)#execute the action\n",
    "        \n",
    "        #update q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.min(q_table[new_state, :]) - q_table[state, action])\n",
    "        #print(n,q_table[state, action],state,action)\n",
    "        state = new_state #set current state to new\n",
    "        rewards_current_episode += reward #update the reward\n",
    "        if done == True: #check if last action ended the episode\n",
    "            break\n",
    "            \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode) #decay exploration rate\n",
    "    rewards_all_episodes.append(rewards_current_episode) #append the episode reward\n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "clt=0\n",
    "\n",
    "print(\"Average reward per thousand episodes\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    clt=clt+sum(r/1000)\n",
    "print(\"Total Score\",clt/(num_episodes/1000))\n",
    "print(\"Average Number of steps per episode\",np.mean(episode_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the Rewards? Why do we choose them\n",
    "A reward in RL is part of the feedback from the environment. When an agent interacts with the environment, he can observe the changes in the state and reward signal through his actions. He can then use this reward signal (can be positive for a good action or negative for a bad action) to draw conclusions about how to behave in a state.The goal, in general, is to solve a given task with the maximum reward possible. For the game of Frozen Lake we get a reward 1 every time the agent reaches the goal and reward 0 if the agent takes falls in the hole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How did you choose your decay rate and starting epsilon? Try at least one additional value for epsilon and the decay rate. How did it change the baseline performance? What is the value of epsilon when if you reach the max steps per episode?\n",
    "The value of epsilon originally takes in 1 as the agent needs to explore the environment as the Q-table is initially 0. The decay rate is small as the reward system is sparse therefore allowing the agent to explore most of the environment before the reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 99 #max number of steps that the agent is allowed to take while playing the game\n",
    "\n",
    "learning_rate = 0.07   # The value of alpha = learning rate in the bellman's equation \n",
    "discount_rate = 0.99     # Dicount rate is the penalty after the moving from one state to the other, in order to complete the game in less number of steps\n",
    "\n",
    "exploration_rate = 1          #Exploration rate\n",
    "max_exploration_rate = 1      #max Exploration Rate\n",
    "min_exploration_rate = 0.01   #Min Exploration Rate\n",
    "exploration_decay_rate = 0.001 #Decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per thousand episodes\n",
      "\n",
      "1000 :  0.04500000000000003\n",
      "2000 :  0.21000000000000016\n",
      "3000 :  0.4070000000000003\n",
      "4000 :  0.5490000000000004\n",
      "5000 :  0.5820000000000004\n",
      "6000 :  0.6700000000000005\n",
      "7000 :  0.6740000000000005\n",
      "8000 :  0.6800000000000005\n",
      "9000 :  0.7020000000000005\n",
      "10000 :  0.6900000000000005\n",
      "Total Score 0.5209000000000004\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []  #hold all the rewards from each episode\n",
    "#q learning algo\n",
    "episode_steps = []\n",
    "for episode in range(num_episodes): #every thing that happens in a episode\n",
    "    state = env.reset() #reset the state of envt to start\n",
    "    done = False #track if the episode is finished\n",
    "    rewards_current_episode = 0 \n",
    "    \n",
    "    for step in range(max_steps_per_episode): #every thing that happens in a single time step in a episode\n",
    "\n",
    "    # Exploration-exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1) #whether the agent eill explore \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:]) # exploit envt\n",
    "        else:\n",
    "            action = env.action_space.sample() #explore\n",
    "        \n",
    "        # action is choosen\n",
    "        new_state, reward, done, info = env.step(action)#execute the action\n",
    "        \n",
    "        #update q table for Q(s,a)\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        #print(n,q_table[state, action],state,action)\n",
    "        state = new_state #set current state to new\n",
    "        rewards_current_episode += reward #update the reward\n",
    "        if done == True: #check if last action ended the episode\n",
    "            break\n",
    "        if step == max_steps_per_episode:\n",
    "            print(exploration_rate)\n",
    "    episode_steps.append(step)        \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "    (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode) #decay exploration rate\n",
    "    rewards_all_episodes.append(rewards_current_episode) #append the episode reward\n",
    "    \n",
    "# Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thosand_episodes = np.split(np.array(rewards_all_episodes),num_episodes/1000)\n",
    "count = 1000\n",
    "clt=0\n",
    "\n",
    "print(\"Average reward per thousand episodes\\n\")\n",
    "for r in rewards_per_thosand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "    clt=clt+sum(r/1000)\n",
    "print(\"Total Score\",clt/(num_episodes/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does Q-learning use value-based or policy-based iteration?\n",
    "Q-Learning uses value based iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is meant by expected lifetime value in the Bellman equation?\n",
    "The expected lifetime value is calculated in the bellman equation by maxQ'(s',a'), as it helps us calculate the expeced final reward, depending on the actions taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://www.youtube.com/watch?v=nyjbcRQ-uQ8&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv\n",
    "- https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# License\n",
    "Copyright 2020 Ashutosh Raichurkar\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "By using Q-Learning we are training the agent to play the game and improve the acuracy by changing the hyper-parameters like Learning Rate, Discount Rate and Decay Rate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
